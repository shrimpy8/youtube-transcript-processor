{"success":true,"summaries":[{"provider":"anthropic","modelName":"Anthropic Sonnet 4.5","summary":"### 1. Tools & Technologies Mentioned\n\n#### **Cursor** (IDE)\n- **Category**: Developer Tools — AI-enhanced IDE\n- **Version**: Not specified\n- **Use case**: Primary coding environment for building production apps; hosts Claude Code and Composer models\n- **Key features**: Multi-model support (Claude, Gemini, Codex), slash commands for reusable prompts, branch-based development, inline code suggestions\n- **Limitations**: Can be intimidating for non-technical users initially (speaker recommends gradual adoption)\n\n#### **Claude** (Anthropic)\n- **Category**: AI/ML — Large Language Model\n- **Version**: Sonnet 3.5 (specifically mentioned as the turning point), Sonnet 3 (mentioned in context of version confusion)\n- **Use case**: Primary coding agent for planning, exploration, code generation, and review\n- **Key features**: Strong at communication, opinionated but collaborative, good at system architecture and planning, supports MCP (Model Context Protocol) for tool integration\n- **Limitations**: Can be overly verbose; needs structured prompts to avoid \"people pleaser\" behavior\n\n#### **Claude Code** (Anthropic)\n- **Category**: AI/ML — Coding Agent\n- **Use case**: Main development agent running within Cursor; handles exploration, planning, execution, and review phases\n- **Key features**: Direct access to codebase, tool-calling via MCP, slash command support, can review its own code\n- **Limitations**: Requires careful prompt engineering to avoid mistakes\n\n#### **Cursor Composer**\n- **Category**: AI/ML — Fast Coding Model\n- **Use case**: Rapid code execution for less complex tasks\n- **Key features**: \"Blazing fast\" execution speed, good for straightforward implementations\n- **Limitations**: Not discussed\n\n#### **Codex** (ChatGPT/OpenAI)\n- **Category**: AI/ML — Code Review Model\n- **Version**: GPT 5.1 Max (speaker notes \"not the best at naming\")\n- **Use case**: Secondary code review agent; used for peer review workflow\n- **Key features**: Excellent at solving difficult bugs, non-communicative but highly effective, best for deep technical issues\n- **Limitations**: Poor at communication; doesn't explain its reasoning well (speaker describes it as \"closes door for two hours, comes back and says 'I fixed it'\")\n\n#### **Gemini 3** (Google)\n- **Category**: AI/ML — UI/Design-Focused Model\n- **Use case**: Front-end and UI design work\n- **Key features**: Exceptional at visual design and UI implementation\n- **Limitations**: \"Terrifying\" workflow — makes erratic decisions during execution (e.g., \"first I'll delete the dashboard... nope, that was a mistake\"), but produces beautiful results\n\n#### **Anti-Gravity** (Google)\n- **Category**: Developer Tools — Cursor competitor\n- **Use case**: Alternative IDE for using Gemini models\n- **Key features**: Shows model's thought process during code generation\n- **Limitations**: Workflow can be anxiety-inducing to watch\n\n#### **Bolt** (StackBlitz)\n- **Category**: No-Code/Low-Code — AI App Builder\n- **Use case**: Early-stage prototyping and learning tool\n- **Key features**: Eager to write code immediately, good for beginners, handles basic features out-of-the-box\n- **Limitations**: Struggles with complex features like payment integration; very opinionated about implementation; speaker \"outgrew it\" after a few months\n\n#### **Lovable**\n- **Category**: No-Code/Low-Code — AI App Builder\n- **Use case**: Early-stage prototyping and learning tool (similar to Bolt)\n- **Key features**: Similar to Bolt — abstracts complexity for beginners\n- **Limitations**: Less control over technical decisions; speaker graduated to Cursor for more serious work\n\n#### **ChatGPT Projects** (OpenAI)\n- **Category**: AI/ML — Project-Based Chat Interface\n- **Use case**: Early workflow tool for creating a \"CTO\" assistant; interview preparation; learning\n- **Key features**: Shared folder of chats with custom instructions and knowledge base, compartmentalized memory (avoids cross-contamination between projects)\n- **Limitations**: Memory feature can mix contexts inappropriately (e.g., mixing running advice with product reviews); \"people pleaser\" behavior (once told speaker that Bun JavaScript was \"the same\" as Zustand when it's completely unrelated, then admitted \"I thought you were just making this up and I was riffing with you\")\n\n#### **Linear**\n- **Category**: Project Management — Issue Tracking\n- **Use case**: Issue creation and tracking via MCP integration with Claude\n- **Key features**: API integration via MCP allows Claude to create and update tickets automatically\n- **Limitations**: Not discussed\n\n#### **MCP (Model Context Protocol)** (Anthropic)\n- **Category**: AI Infrastructure — Tool Integration Protocol\n- **Use case**: Enables AI models to use external tools (e.g., Linear API)\n- **Key features**: Allows Claude to directly interact with external services\n- **Limitations**: Not discussed\n\n#### **Whisperflow**\n- **Category**: Productivity — Voice Dictation\n- **Use case**: Voice-to-text for dictating prompts and commands to AI\n- **Key features**: Enables hands-free coding workflow\n- **Limitations**: Not discussed\n\n#### **Base 44**\n- **Category**: No-Code/Low-Code — AI App Builder\n- **Use case**: Rapid prototyping; speaker built a quiz game for interview prep\n- **Key features**: Handles authentication (sign-in with Google) and database setup automatically\n- **Limitations**: Less control over technical decisions (e.g., which database, which auth flow)\n\n#### **Comet** (Perplexity)\n- **Category**: AI/ML — Browser Agent\n- **Use case**: Web scraping and analysis (speaker used it to analyze interview question frequency from Lewis Lynn's question bank)\n- **Key features**: Can run analyses on web content\n- **Limitations**: Not discussed\n\n#### **Cap**\n- **Category**: Productivity — Screen Recording\n- **Use case**: Loom alternative for screen recording\n- **Key features**: Open-source, well-crafted, free alternative to Loom\n- **Limitations**: Not discussed\n\n#### **Zustand**\n- **Category**: Developer Tools — State Management Framework (JavaScript)\n- **Use case**: State management in speaker's app (mentioned in context of ChatGPT hallucination)\n- **Key features**: Not discussed\n- **Limitations**: Not discussed\n\n#### **Bun JavaScript**\n- **Category**: Developer Tools — JavaScript Runtime\n- **Use case**: Speaker was researching it; mentioned it was \"acquired by Anthropic\" (this may be an error or hallucination example — speaker was describing ChatGPT's mistake)\n- **Key features**: Not discussed\n- **Limitations**: Not discussed\n\n\n\n### 2. Workflows & Processes\n\n#### AI-Assisted Feature Development Workflow\n**Purpose**: End-to-end feature development for non-technical PMs\n\n**Steps**:\n1. **Create Issue** (`/create-issue` slash command)\n   - Invoked mid-development when a bug or feature idea arises\n   - Claude asks brief clarifying questions\n   - Claude uses MCP to create a Linear ticket with TLDR, current state, expected outcomes, and context\n   - **Tip**: \"Quickly capture what I'm thinking about so I can keep working\" — designed to minimize context-switching\n\n2. **Exploration Phase** (`/exploration` slash command)\n   - Invoked when ready to pick up a ticket (e.g., `/exploration linear-88`)\n   - Claude fetches the Linear ticket and reads relevant codebase files\n   - Claude analyzes current architecture, data models, and technical constraints\n   - Claude asks clarifying questions about scope, data model, UX/UI, validation, grading, AI prompts\n   - **Tip**: \"Spend a lot of time going over this because this is super super important\" — don't rush planning\n\n3. **Create Plan** (`/create-plan` slash command)\n   - Claude generates a markdown file with TLDR, critical decisions, and task breakdown\n   - Plan includes status trackers for each task\n   - Plan format: minimal, concise steps with clear status tracking\n   - **Tip**: \"Having this as a markdown file is really good... later on if an agent is writing code in a certain area it can see what's already been done there\"\n\n4. **Execute Plan** (`/execute` slash command or direct instruction)\n   - Tag the plan file and instruct Claude/Composer to execute\n   - Speaker often uses Cursor Composer for speed on straightforward tasks\n   - For complex tasks, uses Claude Code\n   - For front-end/UI work, uses Gemini 3\n   - **Tip**: \"Composer is ridiculously fast\" — use it for non-complex implementations\n\n5. **Manual QA**\n   - Run app locally and test feature manually\n   - Look for obvious bugs or UX issues\n   - **Tip**: Do this before code review to catch surface-level issues\n\n6. **Review** (`/review` slash command)\n   - Claude reviews its own code and identifies bugs (critical, high, medium priority)\n   - **Tip**: \"It's very difficult for me to catch mistakes\" — automate self-review first\n\n7. **Peer Review** (`/peer-review` slash command)\n   - Run `/review` in Codex and Cursor Composer separately\n   - Copy results from each model\n   - Use `/peer-review` in Claude with format: \"Dev Lead 1: [Codex results]\" and \"Dev Lead 2: [Composer results]\"\n   - Claude acts as \"dev lead\" and either fixes issues or explains why they're not real problems\n   - **Tip**: \"They're all going to catch different things\" — models have different strengths in code review\n\n8. **Update Documentation** (`/update-docs` slash command)\n   - Update markdown documentation and tooling based on what was learned\n   - Ensures future agents can write better code in this area\n   - **Tip**: \"Updating documentation and tooling is one of the biggest hacks for productivity\"\n\n**Tools**: Claude Code, Cursor Composer, Codex, Gemini 3, Linear, MCP\n\n**Gotchas**:\n- Don't skip exploration phase — planning prevents gnarly bugs\n- Don't let models write code immediately without planning (Bolt/Lovable were \"too eager\")\n- Run peer review multiple times with different models — they catch different bugs\n- Always update documentation after mistakes to prevent recurrence\n\n\n#### Post-Mortem and Continuous Improvement Workflow\n**Purpose**: Learn from AI mistakes and improve prompts/documentation\n\n**Steps**:\n1. Identify a bug or mistake Claude made\n2. Ask Claude: \"What in your system prompt or tooling made you make this mistake?\"\n3. Claude reflects on root cause\n4. Update documentation, tooling, or slash commands to prevent recurrence\n5. **Tip**: \"Going back and even when you've succeeded, understanding what you did and what you could have done better is critical\"\n\n**Tools**: Claude, Cursor, markdown documentation files\n\n**Gotchas**:\n- Don't just fix the bug and move on — understand the root cause\n- Update the right layer (slash commands, documentation, tooling, or system prompt)\n\n\n#### AI-Assisted Interview Preparation Workflow\n**Purpose**: Prepare for PM interviews (speaker used this for Meta interview)\n\n**Steps**:\n1. Create a ChatGPT Project for interview coaching\n2. Feed it with best resources (speaker used Ben Arez's frameworks)\n3. Use Comet (Perplexity browser) to analyze Lewis Lynn's question bank for most-asked questions\n4. Prioritize mock interviews based on frequency analysis\n5. Conduct mock interviews with ChatGPT\n6. After each mock, ask ChatGPT for feedback (with prompt: \"You're my coach, I don't want you to make me feel good, I want you to make me as ready as possible\")\n7. For questions without time to mock, ask ChatGPT to play the candidate and learn from its \"perfect answer\"\n8. Supplement with human mocks (cold outreach on LinkedIn)\n9. **Tip**: \"The biggest game changer for me was doing human mocks\" — AI is great but humans are essential for final prep\n\n**Tools**: ChatGPT Projects, Comet (Perplexity browser), Base 44 (for quiz game), Ben Arez's frameworks\n\n**Gotchas**:\n- Don't rely solely on AI mocks — human feedback is critical\n- Prime the coach to give tough feedback, not feel-good responses\n\n\n#### Learning Opportunity Workflow\n**Purpose**: Learn technical concepts as a non-technical PM\n\n**Steps**:\n1. Encounter a difficult technical concept during development\n2. Invoke `/learning-opportunity` slash command\n3. Describe what you want to learn\n4. Claude explains using 80/20 rule, assuming \"mid-level engineering knowledge\"\n5. **Tip**: \"Every time you kind of see something that you don't fully understand, I would definitely use this to learn\"\n\n**Tools**: Claude Code, Cursor\n\n**Gotchas**:\n- Don't skip learning opportunities — they compound over time\n\n\n\n### 3. Tips, Techniques & Best Practices\n\n#### General AI Workflow Tips\n- **Graduate tools gradually**: Start with ChatGPT Projects → Bolt/Lovable → Cursor (light mode) → Cursor (dark mode/terminal). Speaker: \"I would really recommend doing this gradually... exposure therapy\"\n- **Think of models as people**: Each model has distinct characteristics. Claude is a \"perfect CTO\" (communicative, opinionated, collaborative). Codex is the \"best coder in a dark room\" (non-communicative but solves hard bugs). Gemini is a \"crazy scientist\" (artsy, great at design, terrifying workflow)\n- **Use multiple models for peer review**: Run code review in Claude, Codex, and Composer separately, then have Claude reconcile differences. Speaker: \"They're all going to catch different things\"\n- **Update documentation after every mistake**: Ask AI what caused the mistake, then update docs/tooling to prevent recurrence. Speaker: \"This is probably one of the biggest unlocks\"\n- **Make your codebase AI-native**: Add markdown files explaining structure and workflows for agents. Speaker: \"My codebase has a ton of just plain text in it\"\n\n#### Prompt Engineering Tips\n- **Avoid \"people pleaser\" behavior**: Tell Claude \"I want you to challenge me. I don't want you to be a people pleaser\" in system prompt\n- **Compartmentalize contexts**: Use ChatGPT Projects to avoid memory cross-contamination (e.g., running advice mixed with product reviews)\n- **Prime for learning, not output**: For interview prep, tell ChatGPT \"You're my coach, I don't want you to make me feel good, I want you to make me as ready as possible\"\n- **Use placeholders in slash commands**: Slash commands can take arguments (e.g., `/exploration linear-88`) for dynamic context\n\n#### Code Review Best Practices\n- **Manual QA first**: Test feature locally before AI code review to catch obvious issues\n- **Self-review before peer review**: Have Claude review its own code first with `/review`\n- **Use peer review for conflicts**: When models disagree, have Claude (as \"dev lead\") reconcile. Claude will sometimes say \"This has been raised for the third time and for the third time I'm telling you this is not an issue\"\n- **Run `/de-slop` in Cursor**: Cursor has a built-in command to remove AI-generated \"slop\" (speaker mentions this is on Twitter, may not be fully integrated yet)\n\n#### Tool Selection Tips\n- **Use Composer for speed**: For straightforward tasks, Composer is \"blazing fast\" and \"keeps you in flow\"\n- **Use Gemini for UI**: \"Gemini is very good at design\" despite its chaotic workflow\n- **Use Codex for hard bugs**: Codex is best for \"the worst bugs\" — non-communicative but highly effective\n- **Split work by model strength**: Backend → Claude, Frontend → Gemini, Fast iterations → Composer\n\n#### Learning and Skill Development\n- **Be a 10x learner, not a 10x PM**: Early in career, focus on learning rate, not output quality. Speaker's manager at Wix: \"They had zero expectation of me being a 10x PM, but the expectation was being a 10x learner\"\n- **Use AI for exposure therapy**: If code is \"terrifying,\" gradually increase exposure via ChatGPT Projects → Bolt → Cursor\n- **Leverage `/learning-opportunity`**: When stuck, use this slash command to get 80/20 explanations at \"mid-level engineering knowledge\"\n- **Learn from AI's perfect answers**: For interview prep, ask ChatGPT to play the candidate and study its response\n\n#### Anti-Patterns to Avoid\n- **Don't let models write code immediately**: Bolt/Lovable are \"super eager to write code\" which causes problems. Always plan first\n- **Don't outsource your thinking**: AI is a tool, not a replacement for judgment. Speaker: \"If you put anything out there and you say 'Oh sorry, that was built by AI,' that's your mistake\"\n- **Don't skip human validation**: For interview prep, \"human mocks\" are essential even with great AI mocks\n- **Don't ignore AI mistakes**: When AI hallucinates or fails, don't just \"keep running at the wall\" — do a post-mortem and update docs\n\n\n\n### 4. Metrics & Numbers\n\n- **Feature development time**: Full features built in \"minutes\" using Cursor Composer (compared to \"days or maybe a week\" for human engineers)\n- **Cost per feature**: Speaker mentioned spending \"a couple bucks in AI credits\" per feature build (exact figure not stated, but implies very low cost)\n- **Localization time**: Fully localized Studymate from Hebrew to English in \"two days\" (speaker notes this \"would probably take a dev team weeks\")\n- **Personal site launch time**: Built and deployed a personal site from \"no domain, no nothing to live on a domain within an hour and a half\"\n- **Thermal clothing profit margin**: As a high school entrepreneur, speaker negotiated price from $20-25 per piece (with $4 profit) down to $1.25 per piece, achieving \"100% profit\" margin\n- **Interview prep timeline**: Speaker prepared for Meta PM interview over several weeks using AI (no specific duration stated)","success":true}]}